{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e975e3",
   "metadata": {},
   "source": [
    "# Custom Artificial Neural Network\n",
    "We'll be building a custom neural network from scratch using simple mathematics and python's Numpy package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea308c",
   "metadata": {},
   "source": [
    "###### What is an Artificial Neural Network(ANN)?\n",
    "An ANN can simply be defined as a computer system designed to emulate the brain in its ability to learn.  \n",
    "As the name suggests, it is the coming together of several neurons to achieve a common purpose.  \n",
    "In this notebook I'll explain in a comprehensive manner how NNs work each step of the way.  \n",
    "<br>\n",
    "We'll train this Neural Network on the famous MNIST dataset. The dataset was downloaded from kaggle. If you wish to procure the data it is available [here](https://www.kaggle.com/competitions/digit-recognizer/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aecfd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bdf050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data into a pandas dataframe.\n",
    "data = pd.read_csv(r\"C:\\Users\\ifunanyaScript\\Everything\\NN_maths_and_numpy/train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd983c0",
   "metadata": {},
   "source": [
    "This is the MNIST dataset. The entire dataset is made up of 60000 samples of handwritten digits however this particular<br>partition of the dataset is only 42000 samples. These samples are 28x28 pixel grayscale images.<br>\n",
    "As we can see above, there are a total of 785 columns including the label column. This is as a result of flattening the 28x28 image to a 1D array of 784 values. Hence, each row represents a sample.  \n",
    "__NB:__ An image can be reconstructed by reshaping the sample by (28, 28)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01670837",
   "metadata": {},
   "source": [
    "The prospective NN cannot understand a pandas dataframe, so we'd have to convert it to a numpy array.  \n",
    "Also we'd get the shape of the data, i.e rows and columns.  \n",
    "Finally, we'll shuffle the data. This is a normal practice to ensure randomness in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af14427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe to a numpy array.\n",
    "data = data.to_numpy()\n",
    "\n",
    "# rows, columns.\n",
    "samples, features = data.shape\n",
    "\n",
    "# Shuffle for randomness.\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e46e542",
   "metadata": {},
   "source": [
    "To make assessment of the data easier, we'll transpose the data, such that each row now becomes a column and each column a row.  \n",
    "\n",
    "When training any machine learning model even ANNs, the goal is always to achieved a superb accuracy on unseen data.  \n",
    "The norms is to spare a little portion of the data at hand for this purpose. Hence, I'll leave out 1000 samples for testing and use the remaining 41000 for training the NN.  \n",
    "\n",
    "Also another important thing to take note of is scaling/normalisation. Each image is made of 784 values that range from 0-255. The magnitude of this range could affect the performance of the NN, so we handle this by scaling the data such that the range is now 0-1; 0 still being black and 1 being white, then decimals between 0 and 1 are shades of gray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40c09384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test chunks, then tanspose.\n",
    "train_data = data[1000:samples].T\n",
    "test_data = data[0:1000].T\n",
    "\n",
    "# Define features(X) and targets(Y) and scale X.\n",
    "X_train = train_data[1:features]\n",
    "X_train = X_train/255.\n",
    "Y_train = train_data[0]\n",
    "\n",
    "X_test = test_data[1:features]\n",
    "X_test = X_test/255.\n",
    "Y_test = test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73031771",
   "metadata": {},
   "source": [
    "Now, to the main event... \n",
    "###### How does a neural network operates internally?\n",
    "We can think of a neural network as a group, working on a specific task to achieve the best result possible.  \n",
    "This group is divided into sub groups, and each sub group is supposed to contribute in a way to achieve the best result. <br>\n",
    "Hypothetically, a group of 100 workers are given a task to build a specific type house. They decide to split themselves into 5 groups.<br> \n",
    "40 people will gather materials, <br>\n",
    "30 people will construct the structure, <br>\n",
    "10 people will mount the roof, <br>\n",
    "10 people will paint the house,<br>\n",
    "And the last 10 people will report to the supervisor.\n",
    "<br>\n",
    "\n",
    "Now, the first four sub groups work on their different tasks and get the house built, then the final group reports to the supervisor.  \n",
    "In neural network this process is called the __Forward propagation__. <br>\n",
    "\n",
    "After the report, the supervisor gives a feedback that, he is not satisfied with the house they have built, and the 10 reporters take this information back to the group. In neural network, this would be the __loss function__<br>\n",
    "\n",
    "With this information, the group takes a look at the individual contributions of each sub group, to find out which ones played a part in the poorly built house.<br> In neural network, this process is called __Backward propagation.__<br>\n",
    "\n",
    "Finally, the group finds where the mistakes are, then makes some ammendments, whether in materials gathered, or structure constructed, or roof mounted, or the colour of paint used, such that the next house they build will better satisfy the supervisor. In neural network this process is called __Gradient descent.__  \n",
    "<br>\n",
    "These four process are carried out respectively and iteratively until the supervisor is satisfied with the house. \n",
    "In neural network, the sum of all these process is called the __Training loop.__<br>\n",
    "\n",
    "This is a perfect summary of neural networks and how they work.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c51c4",
   "metadata": {},
   "source": [
    "We have 10 digits to classify(task),<br>\n",
    "We set up a neural network of 4 layers(sub groups); <br>\n",
    "The first layer is the input layer(material gatherers), <br>\n",
    "The middle layers are the hidden layers(masons, roofers, painters, etc), <br>\n",
    "The final layer is the output layer(reporters).<br>\n",
    "\n",
    "You can go through the above note and noodle it in its entirety to fully understand the concept. <br>  \n",
    "\n",
    "If you've comprehended the previous concept, understanding the math behind neural network is rather simpler than you thought.  \n",
    "A neural network as the name suggest is a network of neurons all working together to achieve the best result.  \n",
    "Now we have a task at hand, which is digit classification. We'll build a neural network that would be able to classify these 10 handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ff6c2",
   "metadata": {},
   "source": [
    "Each sample is a 28x28 pixel image, thus we 784 grayscale values. For this we'll need 784 neurons in the input layer.<br>\n",
    "Then I'll implement only one hidden layer. The number of neurons in this hidden layer is arbitrary, so I'll just choose 10. <br>\n",
    "The last/output layer will contain 10 neurons corresponding to the 10 digits. <br>\n",
    "\n",
    "Now, the most interesting part of the neural network is the training/learning process. But how does the NN learn anything?; there are trainable/learnable parameters called weights and biases.  \n",
    "These weights and biases are numbers used to perform linear operations within a neuron of the network. These linear operation are multiplication for weights and addition for biases.<br>\n",
    "\n",
    "When an image array is passed to the neural network, from the input layer, the 784 pixel values are multiplied by these weights and these bias terms are added to them. This multiplication and addition process is done throughtout the network till the final layer where the classification result is computed using a probability function(softmax). This is the __forward propagation__.<br>\n",
    "\n",
    "Remember I earlier stated that the group working on building a house will try to find out where mistakes are and correct accordingly. Well, after the classification result is computed, the result is compared with the actual label using a loss function. Then with the loss/error, we can calculate how much each weight contributed to the error. This is the __backward propagation__. <br>\n",
    "\n",
    "Now, that we have the gradient of the loss with respect to the weights, we can update the weights appropriately. This is the __gradient descent__.  \n",
    "\n",
    "Recall, this is an iteratively process. The process is carried out until the NN achieves a minimum loss and maximum accuracy. \n",
    "\n",
    "__NB:__ What makes the neural network a powerful architecture is something called activation functions. Activation functions introduce non-linearity into the NN. Without activation functions, the neural network would just be a large and complex linear regression. Hence, the results of each linear operation of weights and biases will be activated using an activation function.<br>Examples of activation functions are ReLU, TanH, LeakyReLU, Sigmoid, Softmax. The first three are usually utilised in hidden layers, while the last two are for output layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eed66f8",
   "metadata": {},
   "source": [
    "At this point, we can start building the neural network.  \n",
    "I earlier mentioned that there are a bunch of mathematical operations that go on within the network. For the purpose of handling these operations we'll define a bunch of functions.  \n",
    "<br>\n",
    "Let's start will all the functions required for the forward propagation.  \n",
    "The first important thing to address is the weights and biases. The initial weights and biases will be randomly generated, and while the network is trained they'll be subsequently optimised.  \n",
    "We can generate these weights using the `rand` function in the numpy package...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5dd37d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly generates the initial weights and biases.\n",
    "def initial_parameters():\n",
    "    first_weights = np.random.rand(10, 784) - .5\n",
    "    first_bias = np.random.rand(10, 1) - .5\n",
    "    second_weights = np.random.rand(10, 10) - .5\n",
    "    second_bias = np.random.rand(10, 1) - .5\n",
    "    return first_weights, first_bias, second_weights, second_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5f8da",
   "metadata": {},
   "source": [
    "You might wonder why I am subtracting __.5__ from the above weights and biases; this is to keep the range of the weights between -0.5 and 0.5, instead of 0-1.  \n",
    "As we go further, I'll explain how these weights multiply our inputs to give an output.  \n",
    "<br>\n",
    "The next function to define is the activation function and the derivative of this activation function.  \n",
    "I will define LeakyReLU and ReLU activation functions, but for the purpose of this project, I think I'll utilise LeakyReLU.  \n",
    "The LeakyReLU is a more robust ReLU function, in the sense that pays a little attention to values that are below zero. This way, the LeakyReLU solves the dying ReLU problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4357b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Leaky ReLU activation.\n",
    "# def Leaky_ReLU(step_1, alpha):\n",
    "#     for sub_array in step_1:\n",
    "#         for i, value in enumerate(sub_array):\n",
    "#             if value>0:\n",
    "#                 sub_array[i] = value\n",
    "#             else:\n",
    "#                 sub_array[i] = (alpha*value)\n",
    "#     return step_1\n",
    "\n",
    "# # Derivative of Leaky ReLU activation.\n",
    "# def LR_deriv(step_1, alpha):\n",
    "#     for sub_array in step_1:\n",
    "#         for i, value in enumerate(sub_array):\n",
    "#             if value>0:\n",
    "#                 sub_array[i] = 1\n",
    "#             else:\n",
    "#                 sub_array[i] = alpha\n",
    "#     return step_1\n",
    "\n",
    "\n",
    "# ReLU activation.\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "# Derivative of ReLU actiavtion.\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00873fbc",
   "metadata": {},
   "source": [
    "Do not trouble yourself with this LeakyReLU function. What you simply need to know is that when the value in a neuron is greater than __0__, the function returns that same value, but when the value is less than __0__, the function returns alpha. Alpha is usually __0.01__.  \n",
    "<br>\n",
    "\n",
    "The next function to define will be the activation function for the last layer.  \n",
    "Now, how will one choose the correct activation for the last layer? Well, this is dependent on the task at hand.  \n",
    "There are two types of classification problems associated with NNs namely; binary classification and multiclass classification.<br>\n",
    "In binary classification, a sigmoid function is utilised at the final layer. Whereas, in multiclass classificatiion, a softmax function is used.  \n",
    "The problem at hand is to classify 10 digits. Apparently, this is a multiclass classification. So we'll use the appropriate activation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1340385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation for output layer.\n",
    "def Softmax(step_3):\n",
    "    prob_array = np.exp(step_3) / sum(np.exp(step_3))\n",
    "    return prob_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d03bf53",
   "metadata": {},
   "source": [
    "The softmax function, takes an array of neurons, i.e values, and returns an array of probability.  \n",
    "These probabilities implies the likelihood of what the input image is. For example, the Softmax function spits out this array: <br>[0.01, 0.98, 0.0, 0.0, 0.03, 0.05, 0.1, 0.0, 0.05, 0.05],<br> As we can see the index containing the highest probability of 98% is the index 1. Thus, the network is suggesting that the input image is digit __1__.  \n",
    "<br>\n",
    "\n",
    "Okay, the next function to define, is a function that one-hot encodes the target label. One-Hot encoding is an approach of representing target labels using zeros and one. This represents labels as arrays of zeros and one. For example, have a look at this: <br>\n",
    "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]<br>\n",
    "We are classifying 10 digits 0-9. The above array means that the target label is __6__, because index 6 has a value of 1 and the rest are zeros.  \n",
    "So we can say one-hot encoding represents the labels as a binary array of zeros and one or as a True(1) or False(0) array..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6492d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot encoder for target labels.\n",
    "def One_Hot_Encoder(Y_train):\n",
    "    binary_labels = np.zeros((Y_train.size, Y_train.max() + 1))\n",
    "    binary_labels[np.arange(Y_train.size), Y_train] = 1\n",
    "    binary_labels = binary_labels.T\n",
    "    return binary_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712cff9",
   "metadata": {},
   "source": [
    "Fantastic!!!<br>\n",
    "We have defined all the functions we need for the forward propagation.  \n",
    "Now we can define the function that will carry out the actual forward propagation.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dba25eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation.\n",
    "def forward_pass(first_weights, first_bias, second_weights, second_bias, X_train):\n",
    "    step_1 = first_weights.dot(X_train) + first_bias\n",
    "    step_2 = ReLU(step_1)\n",
    "    step_3 = second_weights.dot(step_2) + second_bias\n",
    "    step_4 = Softmax(step_3)\n",
    "    return step_1, step_2, step_3, step_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508adb68",
   "metadata": {},
   "source": [
    "I arranged all the processes step by step to aid easier understanding.<br>  \n",
    "In the first step, we perform a dot product operation of the first weights and the 784 input values, then add the first bias terms.  \n",
    "In the second step, we activate the result of the first step using the predefined LeakyReLU function.  \n",
    "In the third step, we perform another dot product operation of the second weights and the resulting values from second step, then add the second bias terms.  \n",
    "In the fourth and final step we activate the result of the third step, to yeild a probability array using the predefined Softmax function.<br>  \n",
    "After the forward propagation, the next issue to address is backward propagation.  \n",
    "After the loss is calculated, the backward propagation is used to get the derivative of each step with respect to the loss.  \n",
    "This is done to know how much of the loss each weight is responsible for.  \n",
    "Let's define the function for backward propagation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f7c22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation.\n",
    "def backward_pass(step_1, step_2, step_3, step_4, first_weights, second_weights, X_train, Y_train):\n",
    "    binary_labels = One_Hot_Encoder(Y_train)\n",
    "    d_step_3 = (step_4 - binary_labels)\n",
    "    d_second_weights = 1/samples * d_step_3.dot(step_2.T)\n",
    "    d_second_bias = 1/samples * np.sum(d_step_3)\n",
    "    d_step_2 = second_weights.T.dot(d_step_3) * ReLU_deriv(step_1)\n",
    "    d_first_weights = 1/samples * d_step_2.dot(X_train.T)\n",
    "    d_first_bias = 1/samples * np.sum(d_step_2)\n",
    "    return d_first_weights, d_first_bias, d_second_weights, d_second_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de68091",
   "metadata": {},
   "source": [
    "This function is basically the reverse of the forward_pass.  \n",
    "The first step(`d_step_3`) is substracting our probability array from the binary labels. This gives us the loss/error of the network<br>\n",
    "The second step(`d_second_weights`), calculates how much the second weight matrice contributed to the overall loss.<br>\n",
    "The third step(`d_second_bias`), calculates how much the second bias terms contributed to the overall loss.<br>\n",
    "The fourth step(`d_step_2`), is essentially calculating the derivative of the activation function.<br>\n",
    "The fifth step(`d_first_weights`), calculates how much the first weight matrice contributed to the overall loss.<br>\n",
    "The sixth step(`d_first_bias`), calculates how much the first bias terms contributed to the overall loss.<br>  \n",
    "Now, all these derivatives can be used to adjust the weights and biases appropriately.  \n",
    "For this we'll define a function that updates the weights and biases...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "270b6a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update function for weights and biases.\n",
    "def update_parameters(first_weights, first_bias, second_weights, second_bias,\n",
    "                      d_first_weights, d_first_bias, d_second_weights, d_second_bias, \n",
    "                      learning_rate):\n",
    "    first_weights = first_weights - d_first_weights*learning_rate\n",
    "    first_bias = first_bias - d_first_bias*learning_rate\n",
    "    second_weights = second_weights - d_second_weights*learning_rate\n",
    "    second_bias = second_bias - d_second_bias*learning_rate\n",
    "    return first_weights, first_bias, second_weights, second_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba55147",
   "metadata": {},
   "source": [
    "This is a simple function. We just subtract gradients from the initial weights.  \n",
    "You might ask, why do we subtract? Well, we subtract so that the weights are moving in the direction of the gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d6076",
   "metadata": {},
   "source": [
    "Essentially, the above functions are all that is needed to build a neural network.  \n",
    "However, we have to assemble all of these functions together to build the actual network. But before we do that, I want to define two functions; one for getting predictions and the other for measuring accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a16d8012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get neural network predictions.\n",
    "def predictions(step_4):\n",
    "    return np.argmax(step_4, 0)\n",
    "\n",
    "# Measure network's accuracy.\n",
    "def accuracy(predictions, Y_train):\n",
    "    return np.sum(predictions == Y_train)/Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa7fc54",
   "metadata": {},
   "source": [
    "Try to wrap your head around the fucntions above, they are pretty straight forward and easy to grasp.<br>  \n",
    "Now, we'll define a giant function that takes all our component functions and build a beautiful neural network.  \n",
    "I'll call this function gradient descent because that the whole process of learning in a neural network, i.e<br>\n",
    "The neural network makes predictions,<br>\n",
    "Calculates the error of the predictions,<br>\n",
    "Get the gradient of the loss with respect to all the weights,<br>\n",
    "Then update the weights, such that the gradient of the loss decreases....\n",
    "__Gradient Descent.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c077bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient for training the network.\n",
    "def gradient_descent(X_train, Y_train, learning_rate, epochs):\n",
    "    first_weights, first_bias, second_weights, second_bias = initial_parameters()\n",
    "    for epoch in range(epochs):\n",
    "        step_1, step_2, step_3, step_4 = forward_pass(first_weights, first_bias,\n",
    "                                                      second_weights, second_bias, X_train)\n",
    "        \n",
    "        d_first_weights, d_first_bias, d_second_weights, d_second_bias = backward_pass(\n",
    "                                                                        step_1, step_2, \n",
    "                                                                        step_3, step_4,\n",
    "                                                                        first_weights,\n",
    "                                                                        second_weights, \n",
    "                                                                        X_train, Y_train)\n",
    "        \n",
    "        first_weights, first_bias, second_weights, second_bias = update_parameters(\n",
    "                                                            first_weights, first_bias, \n",
    "                                                            second_weights, second_bias,\n",
    "                                                            d_first_weights, d_first_bias, \n",
    "                                                            d_second_weights, d_second_bias, \n",
    "                                                            learning_rate)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}/{epochs}\")\n",
    "            pred = predictions(step_4)\n",
    "            print(f\"Accuracy: {(accuracy(pred, Y_train)*100):.2f}%\\n_________________\")\n",
    "    return first_weights, first_bias, second_weights, second_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf8665d",
   "metadata": {},
   "source": [
    "Please take some time to digest the above function.  \n",
    "It's exactly what I described with the 100 workers building a house. The division of labour.<br>\n",
    "\n",
    "The first step is the forward pass were the network makes prediction, equivalent to building the house.<br>\n",
    "The second step is calculating loss and back propagation, equivalent to reporting to the supervisor and finding out where mistakes were made.<br>\n",
    "The third step is updating the weights and bias terms of the network to improve performance and reduce loss, equivalent to the group making ammendments where due to build a better house that satisfies the supervisor.  \n",
    "<br>\n",
    "__NB:__ The above process is carried out iteratively until the accuracy is superb and the loss is negligible, equivalent to the workers, rebuilding the house until the supervisor is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b14436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15000\n",
      "Accuracy: 8.50%\n",
      "_________________\n",
      "Epoch: 100/15000\n",
      "Accuracy: 67.25%\n",
      "_________________\n",
      "Epoch: 200/15000\n",
      "Accuracy: 78.48%\n",
      "_________________\n",
      "Epoch: 300/15000\n",
      "Accuracy: 82.04%\n",
      "_________________\n",
      "Epoch: 400/15000\n",
      "Accuracy: 83.90%\n",
      "_________________\n",
      "Epoch: 500/15000\n",
      "Accuracy: 85.14%\n",
      "_________________\n",
      "Epoch: 600/15000\n",
      "Accuracy: 86.06%\n",
      "_________________\n",
      "Epoch: 700/15000\n",
      "Accuracy: 86.74%\n",
      "_________________\n",
      "Epoch: 800/15000\n",
      "Accuracy: 87.27%\n",
      "_________________\n",
      "Epoch: 900/15000\n",
      "Accuracy: 87.71%\n",
      "_________________\n",
      "Epoch: 1000/15000\n",
      "Accuracy: 88.05%\n",
      "_________________\n",
      "Epoch: 1100/15000\n",
      "Accuracy: 88.35%\n",
      "_________________\n",
      "Epoch: 1200/15000\n",
      "Accuracy: 88.69%\n",
      "_________________\n",
      "Epoch: 1300/15000\n",
      "Accuracy: 88.89%\n",
      "_________________\n",
      "Epoch: 1400/15000\n",
      "Accuracy: 89.10%\n",
      "_________________\n",
      "Epoch: 1500/15000\n",
      "Accuracy: 89.26%\n",
      "_________________\n",
      "Epoch: 1600/15000\n",
      "Accuracy: 89.46%\n",
      "_________________\n",
      "Epoch: 1700/15000\n",
      "Accuracy: 89.63%\n",
      "_________________\n",
      "Epoch: 1800/15000\n",
      "Accuracy: 89.75%\n",
      "_________________\n",
      "Epoch: 1900/15000\n",
      "Accuracy: 89.89%\n",
      "_________________\n",
      "Epoch: 2000/15000\n",
      "Accuracy: 89.97%\n",
      "_________________\n",
      "Epoch: 2100/15000\n",
      "Accuracy: 90.11%\n",
      "_________________\n",
      "Epoch: 2200/15000\n",
      "Accuracy: 90.22%\n",
      "_________________\n",
      "Epoch: 2300/15000\n",
      "Accuracy: 90.31%\n",
      "_________________\n",
      "Epoch: 2400/15000\n",
      "Accuracy: 90.41%\n",
      "_________________\n",
      "Epoch: 2500/15000\n",
      "Accuracy: 90.50%\n",
      "_________________\n",
      "Epoch: 2600/15000\n",
      "Accuracy: 90.60%\n",
      "_________________\n",
      "Epoch: 2700/15000\n",
      "Accuracy: 90.68%\n",
      "_________________\n",
      "Epoch: 2800/15000\n",
      "Accuracy: 90.76%\n",
      "_________________\n",
      "Epoch: 2900/15000\n",
      "Accuracy: 90.83%\n",
      "_________________\n",
      "Epoch: 3000/15000\n",
      "Accuracy: 90.91%\n",
      "_________________\n",
      "Epoch: 3100/15000\n",
      "Accuracy: 90.98%\n",
      "_________________\n",
      "Epoch: 3200/15000\n",
      "Accuracy: 91.06%\n",
      "_________________\n",
      "Epoch: 3300/15000\n",
      "Accuracy: 91.09%\n",
      "_________________\n",
      "Epoch: 3400/15000\n",
      "Accuracy: 91.15%\n",
      "_________________\n",
      "Epoch: 3500/15000\n",
      "Accuracy: 91.22%\n",
      "_________________\n",
      "Epoch: 3600/15000\n",
      "Accuracy: 91.27%\n",
      "_________________\n",
      "Epoch: 3700/15000\n",
      "Accuracy: 91.36%\n",
      "_________________\n",
      "Epoch: 3800/15000\n",
      "Accuracy: 91.44%\n",
      "_________________\n",
      "Epoch: 3900/15000\n",
      "Accuracy: 91.50%\n",
      "_________________\n",
      "Epoch: 4000/15000\n",
      "Accuracy: 91.56%\n",
      "_________________\n",
      "Epoch: 4100/15000\n",
      "Accuracy: 91.61%\n",
      "_________________\n",
      "Epoch: 4200/15000\n",
      "Accuracy: 91.66%\n",
      "_________________\n",
      "Epoch: 4300/15000\n",
      "Accuracy: 91.72%\n",
      "_________________\n",
      "Epoch: 4400/15000\n",
      "Accuracy: 91.76%\n",
      "_________________\n",
      "Epoch: 4500/15000\n",
      "Accuracy: 91.83%\n",
      "_________________\n",
      "Epoch: 4600/15000\n",
      "Accuracy: 91.87%\n",
      "_________________\n",
      "Epoch: 4700/15000\n",
      "Accuracy: 91.94%\n",
      "_________________\n",
      "Epoch: 4800/15000\n",
      "Accuracy: 91.99%\n",
      "_________________\n",
      "Epoch: 4900/15000\n",
      "Accuracy: 92.05%\n",
      "_________________\n",
      "Epoch: 5000/15000\n",
      "Accuracy: 92.12%\n",
      "_________________\n",
      "Epoch: 5100/15000\n",
      "Accuracy: 92.18%\n",
      "_________________\n",
      "Epoch: 5200/15000\n",
      "Accuracy: 92.22%\n",
      "_________________\n",
      "Epoch: 5300/15000\n",
      "Accuracy: 92.25%\n",
      "_________________\n",
      "Epoch: 5400/15000\n",
      "Accuracy: 92.30%\n",
      "_________________\n",
      "Epoch: 5500/15000\n",
      "Accuracy: 92.35%\n",
      "_________________\n",
      "Epoch: 5600/15000\n",
      "Accuracy: 92.40%\n",
      "_________________\n",
      "Epoch: 5700/15000\n",
      "Accuracy: 92.45%\n",
      "_________________\n",
      "Epoch: 5800/15000\n",
      "Accuracy: 92.49%\n",
      "_________________\n",
      "Epoch: 5900/15000\n",
      "Accuracy: 92.51%\n",
      "_________________\n",
      "Epoch: 6000/15000\n",
      "Accuracy: 92.53%\n",
      "_________________\n",
      "Epoch: 6100/15000\n",
      "Accuracy: 92.59%\n",
      "_________________\n",
      "Epoch: 6200/15000\n",
      "Accuracy: 92.65%\n",
      "_________________\n",
      "Epoch: 6300/15000\n",
      "Accuracy: 92.70%\n",
      "_________________\n",
      "Epoch: 6400/15000\n",
      "Accuracy: 92.73%\n",
      "_________________\n",
      "Epoch: 6500/15000\n",
      "Accuracy: 92.76%\n",
      "_________________\n",
      "Epoch: 6600/15000\n",
      "Accuracy: 92.80%\n",
      "_________________\n",
      "Epoch: 6700/15000\n",
      "Accuracy: 92.83%\n",
      "_________________\n",
      "Epoch: 6800/15000\n",
      "Accuracy: 92.86%\n",
      "_________________\n",
      "Epoch: 6900/15000\n",
      "Accuracy: 92.90%\n",
      "_________________\n",
      "Epoch: 7000/15000\n",
      "Accuracy: 92.92%\n",
      "_________________\n",
      "Epoch: 7100/15000\n",
      "Accuracy: 92.94%\n",
      "_________________\n",
      "Epoch: 7200/15000\n",
      "Accuracy: 92.96%\n",
      "_________________\n",
      "Epoch: 7300/15000\n",
      "Accuracy: 92.98%\n",
      "_________________\n",
      "Epoch: 7400/15000\n",
      "Accuracy: 93.02%\n",
      "_________________\n",
      "Epoch: 7500/15000\n",
      "Accuracy: 93.05%\n",
      "_________________\n",
      "Epoch: 7600/15000\n",
      "Accuracy: 93.08%\n",
      "_________________\n",
      "Epoch: 7700/15000\n",
      "Accuracy: 93.10%\n",
      "_________________\n",
      "Epoch: 7800/15000\n",
      "Accuracy: 93.14%\n",
      "_________________\n",
      "Epoch: 7900/15000\n",
      "Accuracy: 93.15%\n",
      "_________________\n",
      "Epoch: 8000/15000\n",
      "Accuracy: 93.20%\n",
      "_________________\n",
      "Epoch: 8100/15000\n",
      "Accuracy: 93.25%\n",
      "_________________\n",
      "Epoch: 8200/15000\n",
      "Accuracy: 93.29%\n",
      "_________________\n",
      "Epoch: 8300/15000\n",
      "Accuracy: 93.34%\n",
      "_________________\n",
      "Epoch: 8400/15000\n",
      "Accuracy: 93.34%\n",
      "_________________\n",
      "Epoch: 8500/15000\n",
      "Accuracy: 93.33%\n",
      "_________________\n",
      "Epoch: 8600/15000\n",
      "Accuracy: 93.34%\n",
      "_________________\n",
      "Epoch: 8700/15000\n",
      "Accuracy: 93.35%\n",
      "_________________\n",
      "Epoch: 8800/15000\n",
      "Accuracy: 93.36%\n",
      "_________________\n",
      "Epoch: 8900/15000\n",
      "Accuracy: 93.37%\n",
      "_________________\n",
      "Epoch: 9000/15000\n",
      "Accuracy: 93.41%\n",
      "_________________\n",
      "Epoch: 9100/15000\n",
      "Accuracy: 93.41%\n",
      "_________________\n",
      "Epoch: 9200/15000\n",
      "Accuracy: 93.44%\n",
      "_________________\n",
      "Epoch: 9300/15000\n",
      "Accuracy: 93.46%\n",
      "_________________\n",
      "Epoch: 9400/15000\n",
      "Accuracy: 93.48%\n",
      "_________________\n",
      "Epoch: 9500/15000\n",
      "Accuracy: 93.49%\n",
      "_________________\n",
      "Epoch: 9600/15000\n",
      "Accuracy: 93.49%\n",
      "_________________\n",
      "Epoch: 9700/15000\n",
      "Accuracy: 93.51%\n",
      "_________________\n",
      "Epoch: 9800/15000\n",
      "Accuracy: 93.53%\n",
      "_________________\n",
      "Epoch: 9900/15000\n",
      "Accuracy: 93.55%\n",
      "_________________\n",
      "Epoch: 10000/15000\n",
      "Accuracy: 93.56%\n",
      "_________________\n",
      "Epoch: 10100/15000\n",
      "Accuracy: 93.56%\n",
      "_________________\n",
      "Epoch: 10200/15000\n",
      "Accuracy: 93.59%\n",
      "_________________\n",
      "Epoch: 10300/15000\n",
      "Accuracy: 93.61%\n",
      "_________________\n",
      "Epoch: 10400/15000\n",
      "Accuracy: 93.63%\n",
      "_________________\n",
      "Epoch: 10500/15000\n",
      "Accuracy: 93.64%\n",
      "_________________\n",
      "Epoch: 10600/15000\n",
      "Accuracy: 93.65%\n",
      "_________________\n",
      "Epoch: 10700/15000\n",
      "Accuracy: 93.66%\n",
      "_________________\n",
      "Epoch: 10800/15000\n",
      "Accuracy: 93.67%\n",
      "_________________\n",
      "Epoch: 10900/15000\n",
      "Accuracy: 93.67%\n",
      "_________________\n",
      "Epoch: 11000/15000\n",
      "Accuracy: 93.68%\n",
      "_________________\n",
      "Epoch: 11100/15000\n",
      "Accuracy: 93.70%\n",
      "_________________\n",
      "Epoch: 11200/15000\n",
      "Accuracy: 93.72%\n",
      "_________________\n",
      "Epoch: 11300/15000\n",
      "Accuracy: 93.75%\n",
      "_________________\n",
      "Epoch: 11400/15000\n",
      "Accuracy: 93.75%\n",
      "_________________\n",
      "Epoch: 11500/15000\n",
      "Accuracy: 93.76%\n",
      "_________________\n",
      "Epoch: 11600/15000\n",
      "Accuracy: 93.79%\n",
      "_________________\n",
      "Epoch: 11700/15000\n",
      "Accuracy: 93.80%\n",
      "_________________\n",
      "Epoch: 11800/15000\n",
      "Accuracy: 93.83%\n",
      "_________________\n",
      "Epoch: 11900/15000\n",
      "Accuracy: 93.85%\n",
      "_________________\n",
      "Epoch: 12000/15000\n",
      "Accuracy: 93.86%\n",
      "_________________\n",
      "Epoch: 12100/15000\n",
      "Accuracy: 93.88%\n",
      "_________________\n",
      "Epoch: 12200/15000\n",
      "Accuracy: 93.87%\n",
      "_________________\n",
      "Epoch: 12300/15000\n",
      "Accuracy: 93.87%\n",
      "_________________\n",
      "Epoch: 12400/15000\n",
      "Accuracy: 93.90%\n",
      "_________________\n",
      "Epoch: 12500/15000\n",
      "Accuracy: 93.92%\n",
      "_________________\n",
      "Epoch: 12600/15000\n",
      "Accuracy: 93.93%\n",
      "_________________\n",
      "Epoch: 12700/15000\n",
      "Accuracy: 93.93%\n",
      "_________________\n",
      "Epoch: 12800/15000\n",
      "Accuracy: 93.94%\n",
      "_________________\n",
      "Epoch: 12900/15000\n",
      "Accuracy: 93.94%\n",
      "_________________\n",
      "Epoch: 13000/15000\n",
      "Accuracy: 93.95%\n",
      "_________________\n",
      "Epoch: 13100/15000\n",
      "Accuracy: 93.96%\n",
      "_________________\n",
      "Epoch: 13200/15000\n",
      "Accuracy: 93.96%\n",
      "_________________\n",
      "Epoch: 13300/15000\n",
      "Accuracy: 93.96%\n",
      "_________________\n",
      "Epoch: 13400/15000\n",
      "Accuracy: 93.98%\n",
      "_________________\n",
      "Epoch: 13500/15000\n",
      "Accuracy: 93.99%\n",
      "_________________\n",
      "Epoch: 13600/15000\n",
      "Accuracy: 94.00%\n",
      "_________________\n",
      "Epoch: 13700/15000\n",
      "Accuracy: 94.02%\n",
      "_________________\n",
      "Epoch: 13800/15000\n",
      "Accuracy: 94.03%\n",
      "_________________\n",
      "Epoch: 13900/15000\n",
      "Accuracy: 94.05%\n",
      "_________________\n",
      "Epoch: 14000/15000\n",
      "Accuracy: 94.07%\n",
      "_________________\n",
      "Epoch: 14100/15000\n",
      "Accuracy: 94.08%\n",
      "_________________\n",
      "Epoch: 14200/15000\n",
      "Accuracy: 94.09%\n",
      "_________________\n",
      "Epoch: 14300/15000\n",
      "Accuracy: 94.10%\n",
      "_________________\n",
      "Epoch: 14400/15000\n",
      "Accuracy: 94.13%\n",
      "_________________\n",
      "Epoch: 14500/15000\n",
      "Accuracy: 94.16%\n",
      "_________________\n",
      "Epoch: 14600/15000\n",
      "Accuracy: 94.19%\n",
      "_________________\n",
      "Epoch: 14700/15000\n",
      "Accuracy: 94.20%\n",
      "_________________\n",
      "Epoch: 14800/15000\n",
      "Accuracy: 94.21%\n",
      "_________________\n"
     ]
    }
   ],
   "source": [
    "# Parameter for training the NN.\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 15000\n",
    "\n",
    "# Training step/loop.\n",
    "first_weights, first_bias, second_weights, second_bias = gradient_descent(X_train, Y_train, LEARNING_RATE, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c632c",
   "metadata": {},
   "source": [
    "Incredible!!!<br>\n",
    "Without any deep learning framework, we have built this custom Artificial Neural Network that achieved over 93% accuracy.<br>\n",
    "Looking at how the accuracy imporved, we can deduce that there is more room for improvement.  \n",
    "At a point, the accuracy increases slowly; you might wonder why? Well, it's because the learning rate is constant.<br>  \n",
    "\n",
    "Remember, the training process is to optimise the initial weights and bias terms.  \n",
    "With this optimised weights, if we carry out the forward pass again, i.e predictions, we should get a high accuracy and a negligible loss.<br>  \n",
    "\n",
    "It's time to actually test the NN and compare its predictions to the actual label.  \n",
    "For this purpose, we'll define two functions.  \n",
    "One, for getting the prediction of the NN.  \n",
    "Two, for comparing the prediction with the actual label.  \n",
    "Also, I'll modify the predefined predefined `predictions` function, so that we can get the confidence of the NN's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2ebcb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified `predictions` to contain confidence.\n",
    "def predictions(step_4):\n",
    "    pred = np.argmax(step_4, 0)\n",
    "    confidence = round(100 * (np.max(step_4)), 2)\n",
    "    return pred, confidence\n",
    "\n",
    "# NN prediction.\n",
    "def test(X, first_weights, first_bias, second_weights, second_bias):\n",
    "    _, _, _, step_4 = forward_pass(first_weights, first_bias, second_weights, second_bias, X)\n",
    "    pred, confidence = predictions(step_4)\n",
    "    return pred, confidence\n",
    "\n",
    "# Validating the prediction.\n",
    "def test_validation(index, first_weights, first_bias, second_weights, second_bias):\n",
    "    image_sample = X_train[:, index, None]\n",
    "    pred, confidence = test(image_sample, first_weights, first_bias, second_weights, second_bias)\n",
    "    label = Y_train[index]\n",
    "    \n",
    "    image_sample = image_sample.reshape((28, 28)) * 255\n",
    "    plt.imshow(image_sample, interpolation=\"nearest\", cmap=\"gray\")\n",
    "    plt.title(f\"Model's prediction: {pred}\\nConfidence: {confidence}%\\nActual label: {label}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48421d3",
   "metadata": {},
   "source": [
    "Assimilate the above functions. They are pretty straight forward.  \n",
    "`test` simply returns the NN's prediction.  \n",
    "`test_validation` compares the NN's prediction and the actual label. And displays the predicted sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f55fe27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_validation(5, first_weights, first_bias, second_weights, second_bias)\n",
    "test_validation(34, first_weights, first_bias, second_weights, second_bias)\n",
    "test_validation(56, first_weights, first_bias, second_weights, second_bias)\n",
    "test_validation(89, first_weights, first_bias, second_weights, second_bias)\n",
    "test_validation(101, first_weights, first_bias, second_weights, second_bias)\n",
    "test_validation(234, first_weights, first_bias, second_weights, second_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5789c",
   "metadata": {},
   "source": [
    "Let's finally see the performance of this neural network on the testing chunk we set out prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "efecebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing data: 93.5%\n"
     ]
    }
   ],
   "source": [
    "predictions, _ = test(X_test, first_weights, first_bias, second_weights, second_bias)\n",
    "print(f\"Accuracy on testing data: {accuracy(predictions, Y_test) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc9c687",
   "metadata": {},
   "source": [
    "Astounding!!!  \n",
    "The network achieved an amazing accuracy on the test data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6521cdb",
   "metadata": {},
   "source": [
    "# Separate testing without labels.\n",
    "As I earlier stated, this dataset was downloaded from kaggle, so there's an entire portion, set out for testing.<br>\n",
    "This particular portion unlabelled, it only has pixel values and it's left for the NN to figure out which digit each image_sample is. <br>  \n",
    "\n",
    "We will validate if the NN is doing well by plotting each sample it predicts. This time around you'll act as the supervisor.  \n",
    "Discern if the network is doing fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7b4e0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_data = pd.read_csv(r\"C:\\Users\\ifunanyaScript\\Everything\\NN_maths_and_numpy\\test.csv\")\n",
    "unlabelled_data.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79951a3",
   "metadata": {},
   "source": [
    "Apparently, there are no labels.\n",
    "Remember, we have to convert the dataframe to a numpy array and also transpose it.  \n",
    "We are transposing the data for __ease of access__. So a column is a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0fefc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 28000), (784, 28000))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_data = unlabelled_data.to_numpy()\n",
    "unlabelled_data = unlabelled_data.T\n",
    "unlabelled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "148ac78b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  10,  17,  17,  17,  17,  81, 180, 180,\n",
       "        35,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 253, 253, 253, 253,\n",
       "       253, 253,  48,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  60, 228, 253, 253, 253,\n",
       "       253, 253, 253, 253, 207, 197,  46,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 213, 253, 253,\n",
       "       253, 253, 253, 253, 253, 253, 253, 253, 223,  52,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  66,\n",
       "       231, 253, 253, 253, 108,  40,  40, 115, 244, 253, 253, 134,   3,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  63, 114, 114, 114,  37,   0,   0,   0, 205, 253, 253,\n",
       "       253,  15,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  57,\n",
       "       253, 253, 253,  15,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,  42, 253, 253, 253,  15,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  95, 253, 253, 253,  15,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 205, 253, 253, 253,  15,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,  61,  99,  96,   0,   0,  45, 224, 253, 253, 195,  10,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11,  25, 105,\n",
       "        83, 189, 189, 228, 253, 251, 189, 189, 218, 253, 253, 210,  27,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  42, 116, 173,\n",
       "       253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253,\n",
       "       221, 116,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0, 118,\n",
       "       253, 253, 253, 253, 245, 212, 222, 253, 253, 253, 253, 253, 253,\n",
       "       253, 253, 253, 253, 160,  15,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0, 254, 253, 253, 253, 189,  99,   0,  32, 202, 253, 253, 253,\n",
       "       240, 122, 122, 190, 253, 253, 253, 174,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0, 255, 253, 253, 253, 238, 222, 222, 222, 241, 253,\n",
       "       253, 230,  70,   0,   0,  17, 175, 229, 253, 253,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 158, 253, 253, 253, 253, 253, 253, 253,\n",
       "       253, 205, 106,  65,   0,   0,   0,   0,   0,  62, 244, 157,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   6,  26, 179, 179, 179, 179,\n",
       "       179,  30,  15,  10,   0,   0,   0,   0,   0,   0,   0,   0,  14,\n",
       "         6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see a sample.\n",
    "unlabelled_data[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eacadd3",
   "metadata": {},
   "source": [
    "Now, we'll define a function that displays an image sample, and gets the Model's prediction of that image sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0169347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function predicts the number using the trained or optimised parameters(weights and biases).\n",
    "def predict_number(index, first_weights, first_bias, second_weights, second_bias):\n",
    "    the_sample = unlabelled_data[:, index, None]\n",
    "    pred = test(unlabelled_data[:, index, None], first_weights, first_bias, second_weights, second_bias)\n",
    "    print(f\"Model's prediction: {pred}\")\n",
    "    \n",
    "    image_sample = the_sample.reshape((28, 28)) * 255\n",
    "    plt.imshow(image_sample, interpolation=\"nearest\", cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2212fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ifunanyaScript\\AppData\\Local\\Temp\\ipykernel_2464\\2666723439.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  prob_array = np.exp(step_3) / sum(np.exp(step_3))\n",
      "C:\\Users\\ifunanyaScript\\AppData\\Local\\Temp\\ipykernel_2464\\2666723439.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prob_array = np.exp(step_3) / sum(np.exp(step_3))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpredict_number\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m23\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m predict_number(\u001b[38;5;241m300\u001b[39m, first_weights, first_bias, second_weights, second_bias)\n\u001b[0;32m      3\u001b[0m predict_number(\u001b[38;5;241m15001\u001b[39m, first_weights, first_bias, second_weights, second_bias)\n",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36mpredict_number\u001b[1;34m(index, first_weights, first_bias, second_weights, second_bias)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_number\u001b[39m(index, first_weights, first_bias, second_weights, second_bias):\n\u001b[0;32m      6\u001b[0m     the_sample \u001b[38;5;241m=\u001b[39m unlabelled_data[:, index, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m----> 7\u001b[0m     pred\u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43munlabelled_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     the_sample \u001b[38;5;241m=\u001b[39m the_sample\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(X, first_weights, first_bias, second_weights, second_bias)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(X, first_weights, first_bias, second_weights, second_bias):\n\u001b[0;32m      9\u001b[0m     _, _, _, step_4 \u001b[38;5;241m=\u001b[39m forward_pass(first_weights, first_bias, second_weights, second_bias, X)\n\u001b[1;32m---> 10\u001b[0m     pred, confidence \u001b[38;5;241m=\u001b[39m predictions(step_4)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred, confidence\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "predict_number(23, first_weights, first_bias, second_weights, second_bias)\n",
    "predict_number(300, first_weights, first_bias, second_weights, second_bias)\n",
    "predict_number(15001, first_weights, first_bias, second_weights, second_bias)\n",
    "predict_number(789, first_weights, first_bias, second_weights, second_bias)\n",
    "predict_number(23456, first_weights, first_bias, second_weights, second_bias)\n",
    "predict_number(3400, first_weights, first_bias, second_weights, second_bias)\n",
    "predict_number(4309, first_weights, first_bias, second_weights, second_bias)\n",
    "predict_number(23787, first_weights, first_bias, second_weights, second_bias)\n",
    "predict_number(27999, first_weights, first_bias, second_weights, second_bias)\n",
    "predict_number(19040, first_weights, first_bias, second_weights, second_bias)\n",
    "predict_number(7802, first_weights, first_bias, second_weights, second_bias)\n",
    "predict_number(9019, first_weights, first_bias, second_weights, second_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca7ac1",
   "metadata": {},
   "source": [
    "Voilà!!! With our eyes we can see our NN is very good.<br>\n",
    "It predicted all the samples we gave it, except the forth sample.<br>\n",
    "Personally, I am unsure about the forth one myself, so.<br>\n",
    "<br>\n",
    "I hope this notebook has enlightened you on a basic level of how neural networks function<br>\n",
    "under the hood. It's all a bunch of maths and more maths, but it's powerful and now you know too 😏😃."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "91845ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ifunanyaScript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96869443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
